1. First of all, I would like to thank you for allowing me to speak in front of you today. My name is Luciano Melodia and I will talk about homological analysis of time series data from power plants.

2. I work at the Friedrich Alexander University Erlangen-Nuremberg and I am pursuing my PhD at the Chair of Evolutionary Data Management. Our project that I am presenting today is a cooperation of our Chair of Informatics 6, i6 for short, and Siemens Energy, who kindly provided us with the power plant data.

3. Let me start with a short description of the upcoming content. Sensor data streams are named using a standardized labeling system. Unfortunately, this is not consistently applied by power plant operators, which is why engineers today manually label sensor data streams when they want to analyze power plant data. This is the reason for our classifier, which is to automate this task. I will also briefly introduce the structure of the technical argument, how homology groups can be used, and why we succeed in training our classifier with representations of homology groups of dimension zero and one. At this point I must assume prior knowledge of persistent homology, but I will be happy to clarify questions about it afterwards. We assume that the time series lie on a 1-manifold, but since this is rather boring for homological analysis, we use an embedding we call Sliding-Window, which depends on two parameters, M and Tau, to represent the time series as a curve within or dense in a hypertorus. Its homology groups are informative and useful for us. Then I will show the architecture of the neural network we use and present the experimental results.

4. Let's start with the classification of power plant sensor data.

5. Here you can see the components of the labeling system. First, the overall plant is described by a letter or a number. Following the same procedure, the functional units, which form a interrelated group of components within a power plant, are encoded and described by an optional number, three letters and two more numbers enumerating the functional units. Aggregates are smaller subsets of functional units. Thus, a hierarchical structure is used to encode the architecture of a power plant in the designations. Finally, two letters and two numbers are used to encode the actual unit to which the sensor is attached. Let me explain this with an example. Here, the entire plant is encoded with the number one. The functional unit is coded as main group 2L, which means the second steam, water and gas circuit. A is the feedwater system and C is the feedwater pump. Finally, the numbers 0 and 3 describe that it is the third feedwater pump of the power plant. The same procedure is followed with the aggregates and operating equipment.

6. Before we come to the theoretical foundations of our approach, I would like to remind you of the so-called manifold assumption. This states that we can assume a one-dimensional manifold, possibly smooth, underlying our set of points that constitute the time series for a sensor. Of course, this manifold would be homeomorphic to the real axis and thus relatively boring in terms of homological investigations. Therefore, we embed the points in a higher-dimensional object whose homological properties encode certain properties of the manifold on which we have conjectured the data.

7. In equation one you can see how the vector for a point looks like depending on two parameters, the step size Tau and the dimension of the embedding space M+1. MTau is then called window size. The set of points associated with the time series T is then defined as as follows. Now the periodicity of the function f(T) has a direct correspondence to the roundness of the embedding of the set of points in Euclidean space of dimension M+1, where a period is defined as shown.  The number of the underlying harmonic components provides information about a suitable embedding dimension, which should be larger than 2N. A similar relationship exists for the number of frequencies, which corresponds to the number of copies of S as factor spaces for the N torus. This means that we can describe fundamental properties of the function of our time series by topological and geometrical properties of a hypertorus, which we can assume to be a manifold under the embedded set of points.

8. I briefly illustrate here the sliding-window embedding of a function f(T) in the corresponding set of points, which is a subset of R M+1.

9. Now I would like to discuss the homological properties of the torus. These allow us to completely describe the hypertorus by just the zeroth and first homology groups, so we can use these as features for our classifiers without having to use higher homology groups and still do sufficient feature extraction. Let T2 be isomorphic to S1 times S1 and let Zp be a field, i.e., Z modulo (pZ), where (pZ) is a maximal prime ideal and p is a prime number. We use the fact that the zeroth homology group of S1 over the field Zp is equal to the first homology group of S1, namely the field Zp itself. Thus the ith homology group of S1 is zero for all i > 1, since homology groups of dimension larger than that of the manifold itself are trivial. For the first homology group of the second torus we obtain two copies of the ground field and the field itself for the homology group of dimension two. Again, all higher homology groups are trivial.

10. The homology groups of a sphere are torsion free. As we work in a ﬁeld of coefficients, we can apply Künneth’s formula, because all modules over a ﬁeld are free. Therefore, we can generalize for the n-dimensional hypertorus as a product space of n copies of the sphere. Its k-th homology group is then given as a direct sum over all indices i-one to i-R, whose sum corresponds to k, of the tensor product of the corresponding i-th homology group of the factor spaces. Note that in the equation only something happens when the indices are 0 or 1, since all higher homology groups of the sphere are zero. This gives the expression of the kth homology group of the torus as Zp to the power of n choose k. We have now established a connection between the dimension of the torus and the homology groups.

11. Let us briefly recall the definition of the kth Betti number, which appear in the persistence diagrams as persistent Betti numbers along the filtration, as the rank of the kth homology group. The Betti numbers for the n-torus, for increasing n, can be taken from this table. We observe that the zeroth Betti number always remains the same, since the torus is path connected. However, the properties of the zeroth homology group are still important as a feature for us, as the persistence diagrams in the zeroth dimension are dependend on the distance function choosen to construct the filtration. Thus, the zeroth persistent homology group gives us information about local connectedness. The first homology group identifies the n-dimensional torus, as it is unique for every n. Therefore we can identify the manifold just by this Betti number.

12. In the next section I will describe our experimental setup, which empirically turned out to work best for our time series.

13. Our neural network consists of three subnetworks. The very first component with batch-normalization is used to process a sample from the raw time seris data. The second and third sub-networks without batch-normalization are used to process the zero and one dimensional Betti curves, a special representation of the zeroth and first persistent homology group, which lie in a Hilbert space. Thus, this representation allows to compute statistics on its given points. We additively combine these inputs before we continue processing the data by LSTM-layers. All layers are implemented using residual connections.

14. Let us talk about results now.

15. They are promising. We wanted to predict the identifiers with the numbering of the individual components, which means that we assigned the signal to the entity of the sensor. We tried to predict only the whole system, the overall system and the functional units, the overall system, the functional units and the aggregate, and the complete identifier. This table can be found in our publication. To predict the total system alone, the neural network without residual connections was better. We could not determine what caused these results.

16. The best classification results are about 64% accuracy for the complete identifiers, about 65% up to the aggregate, 73% up to the functional level and 83% for the overall system. For all experiments it holds that the precision is much bigger then the recall. Thus, the exactness of our classifier is relatively huge in comparison to its average completeness per class. We have shown that residual connections improve classiﬁcation results for all labels except for the overall system assignment. Other than that, we were able to show that using the Betti curves as features for a classifier for each variant of predicting power plant identifiers was superior to classifying time series alone.

17. What have we learned then?

18. Other experiments performed by some of our students show that the OR-entity achieves the highest accuracy in predicting the constituent identiﬁers in all models tested, followed by A, F, and OS. This is promising since we have already demonstrated an accuracy of 83% for OS. Thus, our model might be able to predict the isolated parts of the identifiers with even higher accuracy.

19. Since the signal is embedded in a torus one could construct neural network layers operating on a given Lie group composed as a product space of spheres and euclidean spaces. Applying parallel transport one could work entirely on such manifolds during training and tailor them specificly to the dataset. The required smooth manifold can be
derived from the persistence diagram.

20. Further experiments shall be performed without using the corresponding numbers of the aggregates and functional units. This would result in much higher accuracy and would be suﬃcient for practical use.

I would like to end the presentation with a reference to my other work and to additional sources I have used for this presentation.

Thank you very much for your attention!
